{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61fee253-9c7d-442a-91f4-91ea16747ccc",
   "metadata": {},
   "source": [
    "# Tutorial for the core API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c4957e7-22cc-48a4-bddc-002b47d7a21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66c612e-e270-4972-b1bf-4e69a29f6ead",
   "metadata": {},
   "source": [
    "## Setup parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ed424023-050a-410b-ab5c-e27873c1aa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rashomon import hasse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72baab3-7a61-4c6b-bbdf-426f0eb6cd1b",
   "metadata": {},
   "source": [
    "There are 3 features\n",
    "- Feature 1 takes on four values, {0, 1, 2, 3}\n",
    "- Feature 2 takes on three values, {0, 1, 2}\n",
    "- Feature 3 takes on three values, {0, 1, 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1dd38e7-b89e-40fa-a60a-d8bbeef60ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 3\n",
    "R = np.array([4, 3, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8df1377-3160-4c50-847f-1395771c2449",
   "metadata": {},
   "source": [
    "First, we find all the profiles corresponding to this setup. For the profiles, only the number of features matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "dd169aff-56fe-48af-9ef8-7f2ced8c8d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiles\n",
      "[(0, 0, 0), (0, 0, 1), (0, 1, 0), (0, 1, 1), (1, 0, 0), (1, 0, 1), (1, 1, 0), (1, 1, 1)]\n",
      "\n",
      "Map from each profile tuple to its index in `profiles` list\n",
      "{(0, 0, 0): 0, (0, 0, 1): 1, (0, 1, 0): 2, (0, 1, 1): 3, (1, 0, 0): 4, (1, 0, 1): 5, (1, 1, 0): 6, (1, 1, 1): 7}\n"
     ]
    }
   ],
   "source": [
    "num_profiles = 2**M\n",
    "profiles, profile_map = hasse.enumerate_profiles(M)\n",
    "\n",
    "print(\"Profiles\")\n",
    "print(profiles)\n",
    "\n",
    "print(\"\\nMap from each profile tuple to its index in `profiles` list\")\n",
    "print(profile_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46995908-e9de-4bd9-9d5c-995d437306ab",
   "metadata": {},
   "source": [
    "Next, we find all the possible feature combinations (i.e., policies) in our example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d9164d-6061-4905-9b82-c1e69305a33e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 36 policies\n",
      "[(np.int64(0), np.int64(0), np.int64(0)), (np.int64(0), np.int64(0), np.int64(1)), (np.int64(0), np.int64(0), np.int64(2)), (np.int64(0), np.int64(1), np.int64(0)), (np.int64(0), np.int64(1), np.int64(1)), (np.int64(0), np.int64(1), np.int64(2)), (np.int64(0), np.int64(2), np.int64(0)), (np.int64(0), np.int64(2), np.int64(1)), (np.int64(0), np.int64(2), np.int64(2)), (np.int64(1), np.int64(0), np.int64(0)), (np.int64(1), np.int64(0), np.int64(1)), (np.int64(1), np.int64(0), np.int64(2)), (np.int64(1), np.int64(1), np.int64(0)), (np.int64(1), np.int64(1), np.int64(1)), (np.int64(1), np.int64(1), np.int64(2)), (np.int64(1), np.int64(2), np.int64(0)), (np.int64(1), np.int64(2), np.int64(1)), (np.int64(1), np.int64(2), np.int64(2)), (np.int64(2), np.int64(0), np.int64(0)), (np.int64(2), np.int64(0), np.int64(1)), (np.int64(2), np.int64(0), np.int64(2)), (np.int64(2), np.int64(1), np.int64(0)), (np.int64(2), np.int64(1), np.int64(1)), (np.int64(2), np.int64(1), np.int64(2)), (np.int64(2), np.int64(2), np.int64(0)), (np.int64(2), np.int64(2), np.int64(1)), (np.int64(2), np.int64(2), np.int64(2)), (np.int64(3), np.int64(0), np.int64(0)), (np.int64(3), np.int64(0), np.int64(1)), (np.int64(3), np.int64(0), np.int64(2)), (np.int64(3), np.int64(1), np.int64(0)), (np.int64(3), np.int64(1), np.int64(1)), (np.int64(3), np.int64(1), np.int64(2)), (np.int64(3), np.int64(2), np.int64(0)), (np.int64(3), np.int64(2), np.int64(1)), (np.int64(3), np.int64(2), np.int64(2))]\n"
     ]
    }
   ],
   "source": [
    "all_policies = hasse.enumerate_policies(M, R)\n",
    "num_policies = len(all_policies)\n",
    "\n",
    "print(f\"All {num_policies} policies\")\n",
    "print(all_policies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831ae4d9-717c-4bd7-b91f-30e421410496",
   "metadata": {},
   "source": [
    "## Partition matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12fe8be2-a0b0-47dc-a19a-d4bbe90f9e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rashomon import extract_pools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52cfa17-b660-4bc2-af4e-8f664aa063a1",
   "metadata": {},
   "source": [
    "We will look only at the (1, 1, 1) profile for the purpose of illustration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a5a3c27-079a-4085-bf18-86df2f499e49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(np.int64(1), np.int64(1), np.int64(1)),\n",
       " (np.int64(1), np.int64(1), np.int64(2)),\n",
       " (np.int64(1), np.int64(2), np.int64(1)),\n",
       " (np.int64(1), np.int64(2), np.int64(2)),\n",
       " (np.int64(2), np.int64(1), np.int64(1)),\n",
       " (np.int64(2), np.int64(1), np.int64(2)),\n",
       " (np.int64(2), np.int64(2), np.int64(1)),\n",
       " (np.int64(2), np.int64(2), np.int64(2)),\n",
       " (np.int64(3), np.int64(1), np.int64(1)),\n",
       " (np.int64(3), np.int64(1), np.int64(2)),\n",
       " (np.int64(3), np.int64(2), np.int64(1)),\n",
       " (np.int64(3), np.int64(2), np.int64(2))]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policies_111 = [x for x in all_policies if x[0] > 0 and x[1] > 0 and x[2] > 0]\n",
    "policies_111"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bd74c3-108d-4b7f-9d85-aebb55a0b9da",
   "metadata": {},
   "source": [
    "Let us say that the partition is as follows:\n",
    "- $\\pi_1$ = {(1, 1, 1), (1, 2, 1)}\n",
    "- $\\pi_2$ = {(1, 1, 2), (1, 2, 2)}\n",
    "- $\\pi_3$ = {(2, 1, 1), (2, 2, 1), (3, 1, 1), (3, 2, 1)}\n",
    "- $\\pi_4$ = {(2, 1, 2), (2, 2, 2), (3, 1, 2), (3, 2, 2)}\n",
    "\n",
    "This corresponds to the following $\\Sigma$ matrix. The `np.inf` implies that that feature does not take those factor levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43860f09-3aa8-4c8e-9e4d-46590ec4e4fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.],\n",
       "       [ 1., inf],\n",
       "       [ 0., inf]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigma_111 = np.array([[0, 1],\n",
    "                  [1, np.inf],\n",
    "                  [0, np.inf]])\n",
    "sigma_111"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f239fa0e-32c7-4cc7-bad5-886dd7e736c0",
   "metadata": {},
   "source": [
    "This is how we extract the pools from the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b114782-f1a9-479e-b578-e2466e073d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_pools, pi_policies = extract_pools.extract_pools(policies_111, sigma_111)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566adeff-eb4f-4350-9af2-af0d35e3d2fe",
   "metadata": {},
   "source": [
    "`pi_pools` is a dictionary that maps each pool index to a list of _indices_ of feature combinations in that pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24e3d512-1085-4d51-bbcd-975d04fc02de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [0, 2], 1: [1, 3], 2: [4, 8, 10, 6], 3: [5, 9, 11, 7]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi_pools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794b5a70-362c-43d5-9dec-8e893cab633e",
   "metadata": {},
   "source": [
    "`pi_policies` is a dictionary that maps each feature combination (through its index) to the index of the pool it belongs to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b1b37949-6984-4c0f-96c2-b0cfcfd1aaec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0, 2: 0, 1: 1, 3: 1, 4: 2, 8: 2, 10: 2, 6: 2, 5: 3, 9: 3, 11: 3, 7: 3}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi_policies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4953447d-12b7-44b1-b129-2f868560cc7a",
   "metadata": {},
   "source": [
    "`extract_pools` also has an optional argument `lattice_edges` where you provide the edges in the Hasse. If you call `extract_pools` on the same Hasse very often, it is more efficient to pre-compute the lattice edges once and pass in this argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9972309a-cfd8-4a39-b812-91b2e3abc85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hasse_edges = extract_pools.lattice_edges(policies_111)\n",
    "\n",
    "pi_pools, pi_policies = extract_pools.extract_pools(policies_111, sigma_111, lattice_edges=hasse_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5803252e-432b-4bb2-865f-eaa392faa268",
   "metadata": {},
   "source": [
    "## Generate data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c312868-747c-4e60-9bd9-9958a98e87e7",
   "metadata": {},
   "source": [
    "Since there are 4 pools, we only need to select 4 distributions for the outcome. For simplicity, say the outcomes come from $N(\\mu_{\\pi}, \\sigma_{\\pi}^2)$ with the following parameters for each pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3941e43e-f8f6-47a5-a8e6-fe8419daf40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_111 = np.array([0, 2, 4, -2])\n",
    "var_111 = np.array([1, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dfa68c-f49d-4ef5-a223-0b30cd90d1dd",
   "metadata": {},
   "source": [
    "Fix 50 samples per feature and generate the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a1beed23-81e7-4bc8-94ac-9063e2f2bc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3)\n",
    "\n",
    "num_samples_per_feature = 50\n",
    "num_data = len(policies_111) * num_samples_per_feature\n",
    "\n",
    "X = np.zeros(shape=(num_data, M))\n",
    "D = np.zeros(shape=(num_data, 1), dtype='int_')\n",
    "y = np.zeros(shape=(num_data, 1))\n",
    "\n",
    "idx_ctr = 0\n",
    "for k, feature in enumerate(policies_111):\n",
    "    # policy_idx = [i for i, x in enumerate(all_policies) if x == policy]\n",
    "\n",
    "    pool_id = pi_policies[k]\n",
    "    mu_i = mu_111[pool_id]\n",
    "    var_i = var_111[pool_id]\n",
    "    y_i = np.random.normal(mu_i, var_i, size=(num_samples_per_feature, 1))\n",
    "\n",
    "    start_idx = idx_ctr * num_samples_per_feature\n",
    "    end_idx = (idx_ctr + 1) * num_samples_per_feature\n",
    "\n",
    "    X[start_idx:end_idx, ] = feature\n",
    "    D[start_idx:end_idx, ] = k\n",
    "    y[start_idx:end_idx, ] = y_i\n",
    "\n",
    "    idx_ctr += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80621973-626a-4b14-944a-97e15178cccf",
   "metadata": {},
   "source": [
    "`X` is the feature matrix.\n",
    "\n",
    "`D` tells us the feature indices i.e., `D[i, 0]` is the feature index of `X[i, ]`.\n",
    "\n",
    "`y` is the outcome vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b4eb2400-5658-47e0-b5c3-4a78168a548b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]]\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "[[ 1.78862847]\n",
      " [ 0.43650985]\n",
      " [ 0.09649747]\n",
      " [-1.8634927 ]\n",
      " [-0.2773882 ]\n",
      " [-0.35475898]\n",
      " [-0.08274148]\n",
      " [-0.62700068]\n",
      " [-0.04381817]\n",
      " [-0.47721803]]\n"
     ]
    }
   ],
   "source": [
    "print(X[:10,])\n",
    "\n",
    "print(D[:10])\n",
    "\n",
    "print(y[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6af733-4f9a-48e3-975b-f25b6bd6eefc",
   "metadata": {},
   "source": [
    "We can calculate the mean outcome of each feature through the following object. The first column contains the sums of outcomes for each feature. The second column is the count. So dividing the first column by the second lets us find the average. We keep the sums and counts separately for internal computation purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e67cf696-e873-4b93-94b3-10e925b416b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -14.696341     50.        ]\n",
      " [ 103.83263356   50.        ]\n",
      " [   1.45837656   50.        ]\n",
      " [  95.24720387   50.        ]\n",
      " [ 221.50472005   50.        ]\n",
      " [ -92.80830557   50.        ]\n",
      " [ 189.57330566   50.        ]\n",
      " [-107.31751574   50.        ]\n",
      " [ 209.98734846   50.        ]\n",
      " [-103.75982477   50.        ]\n",
      " [ 200.56828936   50.        ]\n",
      " [ -99.03931665   50.        ]]\n"
     ]
    }
   ],
   "source": [
    "from rashomon import loss\n",
    "\n",
    "policy_means_111 = loss.compute_policy_means(D, y, len(policies_111))\n",
    "print(policy_means_111)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12678a17-d4fe-4fe6-9772-b2d0bd9bcecb",
   "metadata": {},
   "source": [
    "## Finding the Rashomon set for profile (1, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ba6f1fcf-76e2-469c-bd22-57302ca3ed0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rashomon import aggregate\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a15cd0b-9a0e-44c6-acf5-dd0c37e37b6f",
   "metadata": {},
   "source": [
    "Let us set the maximum number of pools to be $H = \\infty$ and the Rashomon threshold to be $\\theta = 8$ and regularization $\\lambda = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9f8eafb3-5703-4c0c-8ddb-dda310488c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "H = np.inf\n",
    "theta = 8\n",
    "lamb = 1\n",
    "\n",
    "RPS_111 = aggregate.RAggregate_profile(M, R, H, D, y, theta, profile=(1, 1, 1), reg=lamb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f98167a-9a14-4cff-99f2-12e5755d25f3",
   "metadata": {},
   "source": [
    "The output of `RAggregate_profile` is an object of type `RashomonSet`. Here are some useful things we can do with this. Observe that the true partition `sigma_111` is the second partition in the RPS and has the least loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "70ff38b7-a2e1-48a7-90be-a2dd5067aa8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Show the partition matrix for each member of the RPS\n",
    "for sig in RPS_111.sigma:\n",
    "    print(sig)\n",
    "\n",
    "# Count the number of pools in each Rashomon partition\n",
    "print(RPS_111.pools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dde88f6f-550e-4730-b162-f5023fdd7c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Calculate the loss\n",
    "# This needs to be done only when calling `RAggregate_profile`\n",
    "# When calling the main function `RAggregate`, loss is automatically calculated\n",
    "RPS_111.calculate_loss(D, y, policies_111, policy_means_111, reg=lamb)\n",
    "\n",
    "# Print the loss for each member in the RPS\n",
    "print(RPS_111.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3751c5-abe8-417c-98b2-958f5d5125fb",
   "metadata": {},
   "source": [
    "Additionally, there is an internal function that manually checks every single partition to see if it belongs to the RPS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7fe09334-1864-44dc-80a6-7b61a99438f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RPS_111_brute_force = aggregate._brute_RAggregate_profile(M, R, H, D, y, theta, profile=(1, 1, 1), reg=lamb)\n",
    "\n",
    "# Verify that the brute force computation matches the branch-and-bound algorithm\n",
    "RPS_111_brute_force.P_hash == RPS_111.P_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53d02fe-1b6a-4d75-bc9e-825dd0494bfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20991547-bbc4-47d0-9ed2-05d9a76fd2f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d050566-a84b-426b-8f5c-7b133fe775cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d41234-b46f-4b33-bb11-1cbce41622db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66e93b52-e679-491d-91bd-535cf696a7ad",
   "metadata": {},
   "source": [
    "## For all profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3647be5-7743-4144-88f9-8f47c425aa42",
   "metadata": {},
   "source": [
    "Fix the partition matrices and outcome parameters for all other profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "08aa0ad7-8f1a-4ac2-90cb-59fe5054d67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile (0, 0, 0)\n",
    "sigma_000 = None\n",
    "mu_000 = np.array([0])\n",
    "var_000 = np.array([1])\n",
    "\n",
    "# Profile (0, 0, 1)\n",
    "sigma_001 = np.array([[1]])\n",
    "mu_001 = np.array([-2])\n",
    "var_001 = np.array([1])\n",
    "\n",
    "# Profile (0, 1, 0)\n",
    "sigma_010 = np.array([[1]])\n",
    "mu_010 = np.array([1])\n",
    "var_010 = np.array([1])\n",
    "\n",
    "# Profile (0, 1, 1)\n",
    "sigma_011 = np.array([[1], [0]])\n",
    "mu_011 = np.array([1, -2])\n",
    "var_011 = np.array([1, 1])\n",
    "\n",
    "# Profile (1, 0, 0)\n",
    "sigma_100 = np.array([[0, 1]])\n",
    "mu_100 = np.array([0, 2])\n",
    "var_100 = np.array([1, 1])\n",
    "\n",
    "# Profile (1, 0, 1)\n",
    "sigma_101 = np.array([[0, 1], [0, np.inf]])\n",
    "mu_101 = np.array([0, 2, 1, -2])\n",
    "var_101 = np.array([1, 1, 1, 1])\n",
    "\n",
    "# Profile (1, 1, 0)\n",
    "sigma_110 = np.array([[0, 1], [1, np.inf]])\n",
    "mu_110 = np.array([0, -2])\n",
    "var_110 = np.array([1, 1])\n",
    "\n",
    "sigma = [sigma_000, sigma_001, sigma_010, sigma_011, sigma_100, sigma_101, sigma_110, sigma_111]\n",
    "mu = [mu_000, mu_001, mu_010, mu_011, mu_100, mu_101, mu_110, mu_111]\n",
    "var = [var_000, var_001, var_010, var_011, var_100, var_101, var_110, var_111]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e3d97a-7344-4f8d-80ea-4b33f3a6bfe0",
   "metadata": {},
   "source": [
    "### Find all pools for each profile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81b041f-f56d-405a-b020-0eaa3770cffe",
   "metadata": {},
   "source": [
    "This code block does what we did previously for a single profile. Since `extract_pools` only works for indexing within a Hasse, we need to carefully map the universal indexing of features across all profiles to its corresponding index within the profile that it belongs to. This is why this code chunk appears more complicated than it actually is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cc7f0144-a6b5-468e-8611-3a50944d97d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "policies_profiles = {}\n",
    "policies_profiles_masked = {}\n",
    "policies_ids_profiles = {}\n",
    "pi_policies = {}\n",
    "pi_pools = {}\n",
    "for k, profile in enumerate(profiles):\n",
    "\n",
    "    policies_temp = [(i, x) for i, x in enumerate(all_policies) if hasse.policy_to_profile(x) == profile]\n",
    "    unzipped_temp = list(zip(*policies_temp))\n",
    "    policies_ids_k = list(unzipped_temp[0])\n",
    "    policies_k = list(unzipped_temp[1])\n",
    "    policies_profiles[k] = deepcopy(policies_k)\n",
    "    policies_ids_profiles[k] = policies_ids_k\n",
    "\n",
    "    profile_mask = list(map(bool, profile))\n",
    "\n",
    "    # Mask the empty arms\n",
    "    for idx, pol in enumerate(policies_k):\n",
    "        policies_k[idx] = tuple([pol[i] for i in range(M) if profile_mask[i]])\n",
    "    policies_profiles_masked[k] = policies_k\n",
    "\n",
    "    if np.sum(profile) > 0:\n",
    "        pi_pools_k, pi_policies_k = extract_pools.extract_pools(policies_k, sigma[k])\n",
    "        if len(pi_pools_k.keys()) != mu[k].shape[0]:\n",
    "            print(f\"Profile {k}. Expected {len(pi_pools_k.keys())} pools. Received {mu[k].shape[0]} means.\")\n",
    "        pi_policies[k] = pi_policies_k\n",
    "        # pi_pools_k has indicies that match with policies_profiles[k]\n",
    "        # Need to map those indices back to all_policies\n",
    "        pi_pools[k] = {}\n",
    "        for x, y in pi_pools_k.items():\n",
    "            y_full = [policies_profiles[k][i] for i in y]\n",
    "            y_agg = [all_policies.index(i) for i in y_full]\n",
    "            pi_pools[k][x] = y_agg\n",
    "    else:\n",
    "        pi_policies[k] = {0: 0}\n",
    "        pi_pools[k] = {0: [0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b22cf3d-1f91-4f14-9c0d-9f0960ff6254",
   "metadata": {},
   "source": [
    "### Generate data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb220e32-9eed-4c80-a7c3-4e118de60289",
   "metadata": {},
   "source": [
    "Again, this repeats what we did for a single profile for all profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0a4f40da-b998-4feb-b474-43c6a6507eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(mu, var, n_per_pol, all_policies, pi_policies, M):\n",
    "    num_data = num_policies * n_per_pol\n",
    "    X = np.zeros(shape=(num_data, M))\n",
    "    D = np.zeros(shape=(num_data, 1), dtype='int_')\n",
    "    y = np.zeros(shape=(num_data, 1))\n",
    "\n",
    "    idx_ctr = 0\n",
    "    for k, profile in enumerate(profiles):\n",
    "        policies_k = policies_profiles[k]\n",
    "\n",
    "        for idx, policy in enumerate(policies_k):\n",
    "            policy_idx = [i for i, x in enumerate(all_policies) if x == policy]\n",
    "\n",
    "            pool_id = pi_policies[k][idx]\n",
    "            mu_i = mu[k][pool_id]\n",
    "            var_i = var[k][pool_id]\n",
    "            y_i = np.random.normal(mu_i, var_i, size=(n_per_pol, 1))\n",
    "\n",
    "            start_idx = idx_ctr * n_per_pol\n",
    "            end_idx = (idx_ctr + 1) * n_per_pol\n",
    "\n",
    "            X[start_idx:end_idx, ] = policy\n",
    "            D[start_idx:end_idx, ] = policy_idx[0]\n",
    "            y[start_idx:end_idx, ] = y_i\n",
    "\n",
    "            idx_ctr += 1\n",
    "\n",
    "    return X, D, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "27329883-5733-4ba3-92fa-61fbc62b3e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples_per_feature = 50000\n",
    "\n",
    "X, D, y = generate_data(mu, var, num_samples_per_feature, all_policies, pi_policies, M)\n",
    "policy_means = loss.compute_policy_means(D, y, num_policies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799e1b79-d311-4c61-a9ea-45c87a0be15d",
   "metadata": {},
   "source": [
    "### Finding the Rashomon Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "27182704-9af8-4888-922e-13f927e22313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0, 0) 12.030462425103009\n",
      "Profile (0, 0, 0) has 1 objects in Rashomon set\n",
      "(0, 0, 1) 12.057744867578162\n",
      "Adaptive\n",
      "Profile (0, 0, 1) took 0.039968013763427734 s adaptively\n",
      "Profile (0, 0, 1) has 2 objects in Rashomon set\n",
      "(0, 1, 0) 12.0582758451157\n",
      "Adaptive\n",
      "Profile (0, 1, 0) took 0.04004096984863281 s adaptively\n",
      "Profile (0, 1, 0) has 2 objects in Rashomon set\n",
      "(0, 1, 1) 12.114337623086165\n",
      "Adaptive\n",
      "Profile (0, 1, 1) took 0.21412301063537598 s adaptively\n",
      "Profile (0, 1, 1) has 4 objects in Rashomon set\n",
      "(1, 0, 0) 12.085844227048296\n",
      "Adaptive\n",
      "Profile (1, 0, 0) took 0.1397690773010254 s adaptively\n",
      "Profile (1, 0, 0) has 4 objects in Rashomon set\n",
      "(1, 0, 1) 12.168943514770383\n",
      "Adaptive\n",
      "Profile (1, 0, 1) took 0.7037448883056641 s adaptively\n",
      "Profile (1, 0, 1) has 8 objects in Rashomon set\n",
      "(1, 1, 0) 12.168701235927644\n",
      "Adaptive\n",
      "Profile (1, 1, 0) took 0.7059371471405029 s adaptively\n",
      "Profile (1, 1, 0) has 8 objects in Rashomon set\n",
      "(1, 1, 1) 12.335497837770925\n",
      "Adaptive\n",
      "Profile (1, 1, 1) took 3.6060731410980225 s adaptively\n",
      "Profile (1, 1, 1) has 16 objects in Rashomon set\n",
      "Finding feasible combinations\n",
      "Min = 11.96975414047457, Max = 36.99717034622853\n"
     ]
    }
   ],
   "source": [
    "H = np.inf\n",
    "theta = 13\n",
    "lamb = 1\n",
    "\n",
    "R_set, R_profiles = aggregate.RAggregate(M, R, H, D, y, theta, reg=lamb, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e1f2ea-e56a-4b80-bbb4-bbdd077be43f",
   "metadata": {},
   "source": [
    "The output of `RAggregate` is different from that of `RAggregate_profile`. For starters, the output is a tuple.\n",
    "\n",
    "The first item `R_set` is a list. Each item in `R_set` is a list itself. The length of this list is the number of profiles. Each item in `R_set[i]` gives an index for a partition of that profile. So `R_set[i][k]` is the partition of the k-th profile in the i-th Rashomon partition in the RPS.\n",
    "\n",
    "The second item `R_profiles` is a list whose length is the number of profiles. Each item is the `RashomonSet` object that we saw earlier. The indices in `R_set` correspond to the partitions in `R_profiles`. So the actual partition of `R_set[i][k]` is retrieved by accessing `R_profiles[k].sigma[R_set[i][k]]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1e6cbb53-c2c4-45af-ac6b-675e20f2aca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 1.,  1.],\n",
       "       [ 1., inf],\n",
       "       [ 1., inf]]), array([[ 1.,  1.],\n",
       "       [ 1., inf],\n",
       "       [ 0., inf]]), array([[ 0.,  1.],\n",
       "       [ 1., inf],\n",
       "       [ 0., inf]]), array([[ 0.,  1.],\n",
       "       [ 1., inf],\n",
       "       [ 1., inf]]), array([[ 1.,  0.],\n",
       "       [ 1., inf],\n",
       "       [ 1., inf]]), array([[ 1.,  1.],\n",
       "       [ 0., inf],\n",
       "       [ 1., inf]])]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_profiles[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a881e443-aeb3-4ccb-83c5-e7a7f893efab",
   "metadata": {},
   "source": [
    "Now, let us see how to access these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6b629a28-fd0b-47e9-8608-64b64797382f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profile (0, 0, 0)\n",
      "Partition\n",
      "None\n",
      "Loss = 1.0276327713315392\n",
      "Number of pools = 1\n",
      "---\n",
      "Profile (0, 0, 1)\n",
      "Partition\n",
      "[[1.]]\n",
      "Loss = 1.0549152190530173\n",
      "Number of pools = 1.0\n",
      "---\n",
      "Profile (0, 1, 0)\n",
      "Partition\n",
      "[[1.]]\n",
      "Loss = 1.0554461937366306\n",
      "Number of pools = 1.0\n",
      "---\n",
      "Profile (0, 1, 1)\n",
      "Partition\n",
      "[[1.]\n",
      " [1.]]\n",
      "Loss = 1.3612916033997204\n",
      "Number of pools = 1.0\n",
      "---\n",
      "Profile (1, 0, 0)\n",
      "Partition\n",
      "[[1. 1.]]\n",
      "Loss = 1.157395257884124\n",
      "Number of pools = 1.0\n",
      "---\n",
      "Profile (1, 0, 1)\n",
      "Partition\n",
      "[[ 1.  1.]\n",
      " [ 1. inf]]\n",
      "Loss = 1.5540328652364819\n",
      "Number of pools = 1.0\n",
      "---\n",
      "Profile (1, 1, 0)\n",
      "Partition\n",
      "[[ 1.  1.]\n",
      " [ 1. inf]]\n",
      "Loss = 1.3142552442601412\n",
      "Number of pools = 1.0\n",
      "---\n",
      "Profile (1, 1, 1)\n",
      "Partition\n",
      "[[ 0.  1.]\n",
      " [ 1. inf]\n",
      " [ 1. inf]]\n",
      "Loss = 4.444784097194952\n",
      "Number of pools = 2.0\n",
      "---\n",
      "Total loss = 12.969753252096606, Total number of pools = 9.0\n"
     ]
    }
   ],
   "source": [
    "i = 3\n",
    "\n",
    "RPS_partitions_i = R_set[i]\n",
    "\n",
    "total_loss = 0\n",
    "total_pools = 0\n",
    "for k, profile in enumerate(profiles):\n",
    "    print(\"Profile\", profile)\n",
    "\n",
    "    R_partition_i_k = R_profiles[k].sigma[RPS_partitions_i[k]]\n",
    "    print(\"Partition\")\n",
    "    print(R_partition_i_k)\n",
    "\n",
    "    # Notice that unlike the per-profile case, the loss of this partition is already pre-computed\n",
    "    loss_i_k = R_profiles[k].loss[RPS_partitions_i[k]]\n",
    "    print(f\"Loss = {loss_i_k}\")\n",
    "    \n",
    "    pools_i_k = R_profiles[k].pools[RPS_partitions_i[k]]\n",
    "    print(f\"Number of pools = {pools_i_k}\")\n",
    "\n",
    "    total_loss += loss_i_k\n",
    "    total_pools += pools_i_k\n",
    "\n",
    "    print(\"---\")\n",
    "\n",
    "print(f\"Total loss = {total_loss}, Total number of pools = {total_pools}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccfcc17-f36c-426c-b9c8-69cf3484a4f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf8fa212-9c7f-4f70-bf00-4c6d9e9798e8",
   "metadata": {},
   "source": [
    "By default `RAggregate` uses only one process. But we can parallelize finding Rashomon sets for each profile by changing the `num_workers` argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "725baf19-c7ce-442f-af3d-04f3373b6bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f7ed10c4-5b0a-4a25-be00-429e222745d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 1 worker, RAggregate took 9.680739164352417 s.\n"
     ]
    }
   ],
   "source": [
    "# num_workers = 1\n",
    "\n",
    "start = time.time()\n",
    "R_set1, R_profiles1 = aggregate.RAggregate(M, R, H, D, y, theta, reg=lamb, num_workers=1)\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "\n",
    "print(f\"With 1 worker, RAggregate took {elapsed} s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f3f67ebd-c1dd-4069-8343-e994d547b949",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process SpawnPoolWorker-7:\n",
      "Process SpawnPoolWorker-8:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/fanyiyang/miniconda3/envs/scGPT/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/fanyiyang/miniconda3/envs/scGPT/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/fanyiyang/miniconda3/envs/scGPT/lib/python3.11/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "                    ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/fanyiyang/miniconda3/envs/scGPT/lib/python3.11/multiprocessing/pool.py\", line 51, in starmapstar\n",
      "    return list(itertools.starmap(args[0], args[1]))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/fanyiyang/Desktop/Research/RHS-X/understanding RPS paper revised/rashomon-partition-sets/Code/rashomon/aggregate/raggregate.py\", line 154, in parallel_worker_RAggregat_profile\n",
      "    rashomon_k = RAggregate_profile(M_k, R_k, H_profile, D_k, y_k, theta_k, profile_k, reg,\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/fanyiyang/Desktop/Research/RHS-X/understanding RPS paper revised/rashomon-partition-sets/Code/rashomon/aggregate/profile.py\", line 86, in RAggregate_profile\n",
      "    if counter.num_pools(sigma) > H:\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/fanyiyang/Desktop/Research/RHS-X/understanding RPS paper revised/rashomon-partition-sets/Code/rashomon/counter.py\", line 277, in num_pools\n",
      "    R = find_R(sigma)\n",
      "        ^^^^^^^^^^^^^\n",
      "  File \"/Users/fanyiyang/Desktop/Research/RHS-X/understanding RPS paper revised/rashomon-partition-sets/Code/rashomon/counter.py\", line 162, in find_R\n",
      "    R = np.sum(~np.isinf(sigma), axis=1) + 2\n",
      "                ^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/fanyiyang/miniconda3/envs/scGPT/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/fanyiyang/miniconda3/envs/scGPT/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/fanyiyang/miniconda3/envs/scGPT/lib/python3.11/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "                    ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/fanyiyang/miniconda3/envs/scGPT/lib/python3.11/multiprocessing/pool.py\", line 51, in starmapstar\n",
      "    return list(itertools.starmap(args[0], args[1]))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/fanyiyang/Desktop/Research/RHS-X/understanding RPS paper revised/rashomon-partition-sets/Code/rashomon/aggregate/raggregate.py\", line 154, in parallel_worker_RAggregat_profile\n",
      "    rashomon_k = RAggregate_profile(M_k, R_k, H_profile, D_k, y_k, theta_k, profile_k, reg,\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/fanyiyang/Desktop/Research/RHS-X/understanding RPS paper revised/rashomon-partition-sets/Code/rashomon/aggregate/profile.py\", line 112, in RAggregate_profile\n",
      "    B = loss.compute_B(D, y, sigma, i, j, policies, policy_means, reg, normalize, hasse_edges)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/fanyiyang/Desktop/Research/RHS-X/understanding RPS paper revised/rashomon-partition-sets/Code/rashomon/loss.py\", line 132, in compute_B\n",
      "    mu_D = predict(D, sigma_fix, policies, policy_means, lattice_edges)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/fanyiyang/Desktop/Research/RHS-X/understanding RPS paper revised/rashomon-partition-sets/Code/rashomon/loss.py\", line 97, in predict\n",
      "    D_pool = [pi_policies[pol_id] for pol_id in D[:, 0]]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/fanyiyang/Desktop/Research/RHS-X/understanding RPS paper revised/rashomon-partition-sets/Code/rashomon/loss.py\", line 97, in <listcomp>\n",
      "    D_pool = [pi_policies[pol_id] for pol_id in D[:, 0]]\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[52]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# num_workers = 2\u001b[39;00m\n\u001b[32m      3\u001b[39m start = time.time()\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m R_set2, R_profiles2 = \u001b[43maggregate\u001b[49m\u001b[43m.\u001b[49m\u001b[43mRAggregate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreg\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlamb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m end = time.time()\n\u001b[32m      6\u001b[39m elapsed = end - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Research/RHS-X/understanding RPS paper revised/rashomon-partition-sets/Code/rashomon/aggregate/raggregate.py:289\u001b[39m, in \u001b[36mRAggregate\u001b[39m\u001b[34m(M, R, H, D, y, theta, reg, verbose, num_workers, bruteforce)\u001b[39m\n\u001b[32m    287\u001b[39m rashomon_profiles: \u001b[38;5;28mlist\u001b[39m[RashomonSet] = [\u001b[38;5;28;01mNone\u001b[39;00m]*num_profiles\n\u001b[32m    288\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Pool(num_workers) \u001b[38;5;28;01mas\u001b[39;00m p:\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m     rashomon_profiles = \u001b[43mp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstarmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparallel_worker_RAggregat_profile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    291\u001b[39m feasible = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    292\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m rashomon_k \u001b[38;5;129;01min\u001b[39;00m rashomon_profiles:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/scGPT/lib/python3.11/multiprocessing/pool.py:375\u001b[39m, in \u001b[36mPool.starmap\u001b[39m\u001b[34m(self, func, iterable, chunksize)\u001b[39m\n\u001b[32m    369\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstarmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    370\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m'''\u001b[39;00m\n\u001b[32m    371\u001b[39m \u001b[33;03m    Like `map()` method but the elements of the `iterable` are expected to\u001b[39;00m\n\u001b[32m    372\u001b[39m \u001b[33;03m    be iterables as well and will be unpacked as arguments. Hence\u001b[39;00m\n\u001b[32m    373\u001b[39m \u001b[33;03m    `func` and (a, b) becomes func(a, b).\u001b[39;00m\n\u001b[32m    374\u001b[39m \u001b[33;03m    '''\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m375\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_map_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstarmapstar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/scGPT/lib/python3.11/multiprocessing/pool.py:768\u001b[39m, in \u001b[36mApplyResult.get\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    767\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m768\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    769\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ready():\n\u001b[32m    770\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/scGPT/lib/python3.11/multiprocessing/pool.py:765\u001b[39m, in \u001b[36mApplyResult.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    764\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m765\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_event\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/scGPT/lib/python3.11/threading.py:629\u001b[39m, in \u001b[36mEvent.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    627\u001b[39m signaled = \u001b[38;5;28mself\u001b[39m._flag\n\u001b[32m    628\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[32m--> \u001b[39m\u001b[32m629\u001b[39m     signaled = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cond\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    630\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/scGPT/lib/python3.11/threading.py:327\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m         \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    328\u001b[39m         gotit = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    329\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# num_workers = 2\n",
    "\n",
    "start = time.time()\n",
    "R_set2, R_profiles2 = aggregate.RAggregate(M, R, H, D, y, theta, reg=lamb, num_workers=2)\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "\n",
    "print(f\"With 2 workers, RAggregate took {elapsed} s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d8bb53-4824-4f38-8bb0-c790a50606aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Check whether the results are the same\n",
    "print(R_set1 == R_set2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e037ab-3cdd-4f91-87f5-425fd8758b7e",
   "metadata": {},
   "source": [
    "The difference of 1 second seems negligible but the gains will be more substantial when there are more features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "5d462e7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_profiles[0].P_qe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ca39ec",
   "metadata": {},
   "source": [
    "### First step: find neighbourhood of a partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "2a5b19fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass\n",
    "from typing import Callable, List, Tuple, Optional, Sequence\n",
    "import numpy as np\n",
    "import math, random\n",
    "\n",
    "# --- bring your repo primitives (edit path as needed) ---\n",
    "# compute_Q(D, y, sigma, policies, policy_means, reg=1, normalize=0, lattice_edges=None) -> float\n",
    "from rashomon.loss import compute_Q\n",
    "from rashomon.aggregate.raggregate import RAggregate          # (variant A)\n",
    "from rashomon import hasse\n",
    "\n",
    "###\n",
    "# 1) State + anchor materialization\n",
    "###\n",
    "\n",
    "class ProfilePart:\n",
    "    cov_ids: List[int]        # active covariate indices for THIS profile\n",
    "    B: np.ndarray  \n",
    "\n",
    "    def __init__(self, cov_ids, B):\n",
    "        self.cov_ids = cov_ids\n",
    "        self.B = B\n",
    "\n",
    "State = List[ProfilePart]  # length P; each item is an (M,R) int matrix\n",
    "\n",
    "def build_anchor_states(R_set, R_profiles, M: int, R: int) -> List[State]:\n",
    "    \"\"\"\n",
    "    R_set[g]: list[int] of length P (which candidate per profile)\n",
    "    R_profiles[p][i]: (M,R) matrix for profile p, candidate i\n",
    "    Returns: list of states; each state is [Sigma_0, ..., Sigma_{P-1}], Sigma_p ∈ ℤ^{M×R}.\n",
    "    \"\"\"\n",
    "    P = len(R_profiles)\n",
    "    anchors: List[State] = []\n",
    "    profiles, profile_map = hasse.enumerate_profiles(int(math.log2(P)))\n",
    "    for idx_vec in R_set:\n",
    "        if len(idx_vec) != P:\n",
    "            raise ValueError(f\"R_set entry length {len(idx_vec)} != P={P}\")\n",
    "        state = []\n",
    "        for p, sel in enumerate(idx_vec):\n",
    "            cand = R_profiles[p].sigma[int(sel)]\n",
    "            state.append(ProfilePart(cov_ids=profiles[p], B=cand))\n",
    "        anchors.append(state)\n",
    "        # # dedup by bytes of concatenated matrices\n",
    "        # key = b\"\".join(S.tobytes() for S in state)\n",
    "        # if key not in seen:\n",
    "        #     anchors.append([S.copy() for S in state])\n",
    "        #     seen.add(key)\n",
    "    return anchors\n",
    "\n",
    "# ---------- One-boundary shifts (UBS) inside a profile ----------\n",
    "# def _row_segments_from_boundaries(b: np.ndarray):\n",
    "#     R = len(b)\n",
    "#     start = 0\n",
    "#     end = R-1\n",
    "#     segs=[]\n",
    "#     for i in range(R):\n",
    "#         if b[i] == 1: \n",
    "#             segs.append((start, i))\n",
    "#             start = i+1\n",
    "#             if (start == R): \n",
    "#                 segs.append((R,R))\n",
    "#                 return segs\n",
    "#         if i == R-1:\n",
    "#             segs.append((start, R))\n",
    "#             return segs\n",
    "        \n",
    "\n",
    "def _unit_shift_row_neighbors(b: np.ndarray) -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Given a boundary row b (0/1, length R), return all neighbors formed by\n",
    "    moving a single 1 one position left or right IF the destination is 0.\n",
    "    No min-segment logic; exactly the 'check-adjacent-zero then move' rule.\n",
    "    \"\"\"\n",
    "    b = np.asarray(b, dtype=int).ravel()\n",
    "    R = b.size\n",
    "    neigh = []\n",
    "    ones = np.where(b == 1)[0]\n",
    "    for j in ones:\n",
    "        # move left\n",
    "        if j - 1 >= 0 and b[j - 1] == 0:\n",
    "            nb = b.copy()\n",
    "            nb[j] = 0\n",
    "            nb[j - 1] = 1\n",
    "            neigh.append(nb)\n",
    "        # move right\n",
    "        if j + 1 < R and b[j + 1] == 0:\n",
    "            nb = b.copy()\n",
    "            nb[j] = 0\n",
    "            nb[j + 1] = 1\n",
    "            neigh.append(nb)\n",
    "    # dedup in case two different moves produce same result (unlikely here)\n",
    "    uniq = {}\n",
    "    for r in neigh:\n",
    "        uniq[r.tobytes()] = r\n",
    "    return list(uniq.values())\n",
    "\n",
    "def profile_part_neighbors_ubs(part: ProfilePart, min_len=1) -> List[ProfilePart]:\n",
    "    C, R = part.B.shape\n",
    "    out=[]\n",
    "    for r in range(C):\n",
    "        b = part.B[r,:]\n",
    "        while math.isinf(b[-1]): # get rid of inf to compute boundary\n",
    "            b = b[:-1]\n",
    "        if len(b)<=1: continue\n",
    "        \n",
    "        for nb_row in _unit_shift_row_neighbors(b):\n",
    "            if nb_row is None: continue\n",
    "            nbB = part.B.copy()\n",
    "            while (len(nbB[r,:]) > len(nb_row)): # pad inf back to keep format\n",
    "                nb_row.append(float('inf'))\n",
    "            nbB[r,:] = nb_row\n",
    "            out.append(ProfilePart(cov_ids=list(part.cov_ids), B=nbB))\n",
    "    return out\n",
    "\n",
    "def state_neighbors_ubs(state: List[ProfilePart], min_len=1) -> List[State]: # state: list of matrix in the order of profiles\n",
    "    neigh=[]\n",
    "    y = state\n",
    "        \n",
    "    for p, profile_state in enumerate(state):\n",
    "        if p == 0:\n",
    "            continue\n",
    "        part = profile_state # the profile and partition we are focusing on\n",
    "        for nb in profile_part_neighbors_ubs(part, min_len=min_len): # enumerate all neighbor of this profile and partition\n",
    "            y_new = y\n",
    "            y_new[p]=nb\n",
    "            neigh.append(y_new)\n",
    "    return neigh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "58c6be56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 1]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 2]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 3]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 4]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 5]),\n",
       " array([0, 0, 0, 0, 0, 0, 1, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 1, 1]),\n",
       " array([0, 0, 0, 0, 0, 0, 2, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 3, 0]),\n",
       " array([0, 0, 0, 0, 0, 1, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 1, 0, 1]),\n",
       " array([0, 0, 0, 0, 0, 2, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 2, 0, 1]),\n",
       " array([0, 0, 0, 0, 0, 3, 0, 0]),\n",
       " array([0, 0, 0, 0, 1, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 1, 0, 0, 1]),\n",
       " array([0, 0, 0, 0, 2, 0, 0, 0]),\n",
       " array([0, 0, 0, 1, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 1, 0, 0, 0, 1]),\n",
       " array([0, 0, 0, 2, 0, 0, 0, 0]),\n",
       " array([0, 0, 1, 0, 0, 0, 0, 0]),\n",
       " array([0, 1, 0, 0, 0, 0, 0, 0])]"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchor = build_anchor_states(R_set, R_profiles, M, R)\n",
    "\n",
    "state_neighbors_ubs(anchor[2])[0][7].B\n",
    "R_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1e13cc",
   "metadata": {},
   "source": [
    "### Second step: AIS, proposal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312a788c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- q0, MH, AIS on this state space ----------\n",
    "def states_equal(a: State, b: State) -> bool:\n",
    "    if len(a)!=len(b): return False\n",
    "    for ap,bp in zip(a,b):\n",
    "        if ap.cov_ids != bp.cov_ids: return False\n",
    "        if not np.array_equal(ap.B, bp.B): return False\n",
    "    return True\n",
    "\n",
    "def Kh_logpdf_state_stay_or_step(x: State, a: State, p_stay=0.25, min_len=1):\n",
    "    if states_equal(x,a): return math.log(p_stay)\n",
    "    N = state_neighbors_ubs(a, min_len=min_len)\n",
    "    for n in N:\n",
    "        if states_equal(x,n):\n",
    "            return math.log(1.0-p_stay) - math.log(len(N))\n",
    "    return float(\"-inf\")\n",
    "\n",
    "def Kh_sample_state_stay_or_step(a: State, rng: np.random.Generator, p_stay=0.25, min_len=1):\n",
    "    if rng.random() < p_stay:  # stay\n",
    "        return [ProfilePart(cov_ids=p.cov_ids, B=(p.B.copy() if p.B is not None else None)) for p in a]\n",
    "    N = state_neighbors_ubs(a, min_len=min_len)\n",
    "    if not N:\n",
    "        return [ProfilePart(cov_ids=p.cov_ids, B=(p.B.copy() if p.B is not None else None)) for p in a]\n",
    "    return N[rng.integers(0,len(N))]\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class AnchorMixture:\n",
    "    anchors: List[State]\n",
    "    log_alpha: List[float]\n",
    "    p_stay: float = 0.25\n",
    "    min_len: int = 1\n",
    "    def __post_init__(self):\n",
    "        m = max(self.log_alpha) if self.log_alpha else 0.0\n",
    "        w = [math.exp(z-m) for z in self.log_alpha] or [1.0]\n",
    "        Z = sum(w) or 1.0\n",
    "        self.alpha = [wi/Z for wi in w]\n",
    "    def log_q0(self, x: State) -> float:\n",
    "        terms=[]\n",
    "        for a,A in zip(self.alpha, self.anchors):\n",
    "            if a==0: continue\n",
    "            lk = Kh_logpdf_state_stay_or_step(x, A, p_stay=self.p_stay, min_len=self.min_len)\n",
    "            if lk==float(\"-inf\"): continue\n",
    "            terms.append(math.log(a)+lk)\n",
    "        if not terms: return float(\"-inf\")\n",
    "        m=max(terms); return m+math.log(sum(math.exp(z-m) for z in terms))\n",
    "    def sample_q0(self, rng: np.random.Generator) -> State:\n",
    "        idx = rng.choice(len(self.anchors), p=np.array(self.alpha))\n",
    "        return Kh_sample_state_stay_or_step(self.anchors[idx], rng, p_stay=self.p_stay, min_len=self.min_len)\n",
    "\n",
    "def make_ladder(K:int, gamma:float=4.0):\n",
    "    if K<2: return np.array([1.0], float)\n",
    "    g=np.linspace(0.0,1.0,K); return g**gamma\n",
    "\n",
    "@dataclass\n",
    "class AISConfig:\n",
    "    n_paths:int=600\n",
    "    n_levels:int=40\n",
    "    moves_per_level:int=12\n",
    "    min_len:int=1\n",
    "    seed:Optional[int]=2\n",
    "\n",
    "@dataclass\n",
    "class AISOutput:\n",
    "    terminals: List[State]\n",
    "    logw: np.ndarray\n",
    "    normw: np.ndarray\n",
    "    ladder: np.ndarray\n",
    "\n",
    "def mh_step_state_uniform_neighbors(x: State, t: float, log_q0, score_s, min_len=1) -> State:\n",
    "    N_cur = state_neighbors_ubs(x, min_len=min_len)\n",
    "    if not N_cur: return x\n",
    "    prop = random.choice(N_cur) # random proposal from the neighbourhood\n",
    "    N_prop = state_neighbors_ubs(prop, min_len=min_len) # neighbourhood of the proposed state\n",
    "    def logpi_t(z: State): # target distribution at this specifc t\n",
    "        lq0 = log_q0(z)\n",
    "        if lq0==float(\"-inf\"): return float(\"-inf\")\n",
    "        return (1.0-t)*lq0 + t*score_s(z) # formula for target dist at ladder step t\n",
    "    lcur, lprop = logpi_t(x), logpi_t(prop) # pi_t of x and proposed \n",
    "    if lprop == float(\"-inf\"): return x\n",
    "    logr = (lprop-lcur) + math.log(max(1,len(N_cur))) - math.log(max(1,len(N_prop))) # acceptance ratio\n",
    "    if math.log(random.random()+1e-300) < min(0.0, logr): return prop\n",
    "    return x\n",
    "\n",
    "def run_ais_state(anchors: List[State], score_s: Callable[[State], float], cfg: AISConfig=AISConfig()) -> AISOutput:\n",
    "    if cfg.seed is not None:\n",
    "        np.random.seed(cfg.seed); random.seed(cfg.seed)\n",
    "    rng=np.random.default_rng(cfg.seed)\n",
    "    log_alpha=[score_s(A) for A in anchors]\n",
    "    mix=AnchorMixture(anchors, log_alpha, p_stay=0.25, min_len=cfg.min_len)\n",
    "    ladder = make_ladder(cfg.n_levels)\n",
    "    terminals=[]; logw=np.zeros(cfg.n_paths,float)\n",
    "    for p in range(cfg.n_paths):\n",
    "        x = mix.sample_q0(rng); lw=0.0 # start with initial sample from anchor\n",
    "        for t_prev, t_cur in zip(ladder[:-1], ladder[1:]):\n",
    "            lq0_x = mix.log_q0(x); s_x = score_s(x) # calculate relevant posterior for specific x\n",
    "            if lq0_x==float(\"-inf\"):\n",
    "                x=mix.sample_q0(rng); lq0_x=mix.log_q0(x); s_x=score_s(x)\n",
    "            lw += (t_cur - t_prev) * (s_x - lq0_x)\n",
    "            for _ in range(cfg.moves_per_level):\n",
    "                x = mh_step_state_uniform_neighbors(x, t_cur, mix.log_q0, score_s, min_len=cfg.min_len)\n",
    "        terminals.append([ProfilePart(cov_ids=pp.cov_ids, B=(pp.B.copy() if pp.B is not None else None)) for pp in x])\n",
    "        logw[p]=lw\n",
    "    m=float(np.max(logw)); w=np.exp(logw-m); return AISOutput(terminals, logw, w/w.sum(), ladder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "df0034d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Any, Sequence, Optional, Union\n",
    "from rashomon.loss import compute_Q as _compute_Q\n",
    "\n",
    "# ---------- 1) Q when sigma is None: one-pool predictor (mean of y_k) ----------\n",
    "def _compute_Q_none_mean(D_k: np.ndarray, y_k: np.ndarray, *, reg: float, normalize: int) -> float:\n",
    "    yv = y_k.ravel()\n",
    "    if yv.size:\n",
    "        mu = float(np.mean(yv))\n",
    "        mse = float(np.mean((yv - mu) ** 2))\n",
    "        if normalize:\n",
    "            mse = mse * yv.size / float(normalize)\n",
    "    else:\n",
    "        mse = 0.0\n",
    "    h = 1  # one pool\n",
    "    return mse + reg * h\n",
    "\n",
    "# ---------- 2) Global loss: sum per-profile losses; DO NOT touch non-None sigma ----------\n",
    "RLike = Union[int, Sequence[int], np.ndarray]\n",
    "\n",
    "def global_loss_raw(\n",
    "    state: Sequence[Any],          # per profile: entry has .B OR is the sigma object itself OR None\n",
    "    D: np.ndarray,                 # shape (N,1): [policy_id]\n",
    "    y: np.ndarray,                 # shape (N,) or (N,1)\n",
    "    policies: Sequence[Any],       # length P\n",
    "    policy_means: np.ndarray,      # shape (P,2)\n",
    "    reg: float = 1.0,\n",
    "    normalize: int = 0,\n",
    "    lattice_edges=None,\n",
    ") -> float:\n",
    "    total = 0.0\n",
    "    P = len(state)\n",
    "    for k in range(P):\n",
    "        sigma_k = getattr(state[k], \"B\", state[k])  # DO NOT sanitize; pass through as-is\n",
    "        mask = (D[:, 0].astype(int) == int(k))\n",
    "        if not np.any(mask):\n",
    "            # No rows for this profile -> treat as one pool (sigma=None logic)\n",
    "            Q_k = _compute_Q_none_mean(D_k=D[:0], y_k=y[:0], reg=reg, normalize=normalize)\n",
    "            total += Q_k\n",
    "            continue\n",
    "\n",
    "        D_k = D[mask].copy()\n",
    "        y_k = y[mask].copy()\n",
    "        D_k[:, 0] = 0  # localize to single-policy view\n",
    "\n",
    "        policies_k = [policies[k]]\n",
    "        pm_k = np.asarray(policy_means[k]).reshape(1, 2)\n",
    "\n",
    "        if sigma_k is None:\n",
    "            Q_k = _compute_Q_none_mean(D_k=D_k, y_k=y_k, reg=reg, normalize=normalize)\n",
    "        else:\n",
    "            # Pass sigma_k EXACTLY as provided (0/1/inf allowed)\n",
    "            Q_k = float(_compute_Q(\n",
    "                D=D_k, y=y_k, sigma=sigma_k,\n",
    "                policies=policies_k, policy_means=pm_k,\n",
    "                reg=reg, normalize=normalize, lattice_edges=lattice_edges\n",
    "            ))\n",
    "        total += Q_k\n",
    "    return total\n",
    "\n",
    "# ---------- 3) AIS score: exp(-beta * global_loss_raw(state)) ----------\n",
    "def make_score_s_expneg_raw(\n",
    "    *,\n",
    "    D,\n",
    "    y,\n",
    "    policies,\n",
    "    policy_means,\n",
    "    reg: float = 1.0,\n",
    "    normalize: int = 0,\n",
    "    lattice_edges=None,\n",
    "    beta: float = 1.0,                 # set 1.0 for exp(-loss)\n",
    "    prior_logprob=lambda state: 0.0,   # optional log-prior\n",
    "):\n",
    "    def score_s(state):\n",
    "        Q = global_loss_raw(\n",
    "            state=state,\n",
    "            D=D, y=y,\n",
    "            policies=policies,\n",
    "            policy_means=policy_means,\n",
    "            reg=reg, normalize=normalize,\n",
    "            lattice_edges=lattice_edges,\n",
    "        )\n",
    "        return float(np.exp(prior_logprob(state) - beta * Q))\n",
    "    return score_s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "2a4b6175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# anchors: list[ProfilePart] per profile (your state format)\n",
    "anchors = build_anchor_states(R_set, R_profiles, M, R)\n",
    "\n",
    "# build the scorer for AIS\n",
    "score_s = make_score_s_expneg_raw(\n",
    "    D=D,\n",
    "    y=y,\n",
    "    policies=all_policies,\n",
    "    policy_means=policy_means,\n",
    "    reg=lamb,\n",
    "    lattice_edges=None,   # or your edges\n",
    "    beta=1.0,             # exp(-loss)\n",
    "    prior_logprob=lambda state: 0.0\n",
    ")\n",
    "\n",
    "# then pass score_s into your AIS routine\n",
    "# ais_out = run_ais_state(anchors, score_s=score_s, cfg=cfg)\n",
    "\n",
    "\n",
    "cfg = AISConfig(n_paths=600, n_levels=40, moves_per_level=12, min_len=1, seed=3)\n",
    "ais_out = run_ais_state(anchors, score_s=score_s, cfg=cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "90805b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.002010050345678765)"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(ais_out.normw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "7abbef18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "\n",
    "def save_ais_pickle(path: str, particles: List[List[ProfilePart]], weights, meta: dict | None = None):\n",
    "    \"\"\"\n",
    "    path: e.g. 'ais_run.pkl' or 'ais_run.pkl.gz' (gzip if endswith .gz)\n",
    "    particles: list of states; each state = list[ProfilePart]\n",
    "    weights: array-like of floats (same length as particles)\n",
    "    meta: optional dict with run info (seed, ladder, etc.)\n",
    "    \"\"\"\n",
    "    payload = {\n",
    "        \"particles\": particles,\n",
    "        \"weights\": np.asarray(weights, float),\n",
    "        \"meta\": meta or {}\n",
    "    }\n",
    "    opener = gzip.open if path.endswith(\".gz\") else open\n",
    "    with opener(path, \"wb\") as f:\n",
    "        # Use highest protocol for speed/size\n",
    "        pickle.dump(payload, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_ais_pickle(path: str):\n",
    "    \"\"\"Returns (particles, weights, meta).\"\"\"\n",
    "    opener = gzip.open if path.endswith(\".gz\") else open\n",
    "    with opener(path, \"rb\") as f:\n",
    "        obj = pickle.load(f)\n",
    "    return obj[\"particles\"], obj[\"weights\"], obj.get(\"meta\", {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "4f5dc655",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_ais_pickle(\"ais_particles.pkl.gz\", ais_out.terminals, ais_out.normw,\n",
    "                meta={\"run_id\":\"first_trial\",\"n_paths\":600,\"n_levels\":40,\"moves_per_level\":12,\"reg\":1,\"seed\":3,\"min_length\":1})\n",
    "\n",
    "particles2, weights2, meta2 = load_ais_pickle(\"ais_particles.pkl.gz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "fc984c9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'run_id': 'first_trial',\n",
       " 'n_paths': 600,\n",
       " 'n_levels': 40,\n",
       " 'moves_per_level': 12,\n",
       " 'reg': 1,\n",
       " 'seed': 3,\n",
       " 'min_length': 1}"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scGPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
